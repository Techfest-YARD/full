# Nazwa Projektu: Let Me Solo Her team

## Spis Treści
- **Opis Projektu**
- **Instrukcje Uruchomienia**
  - **Budowanie Obrazu Docker**
  - **Uruchomienie na GCP**
- **Instrukcja Użytkowania**
- **Diagram Architektury i Opis**
- **Kluczowe Decyzje**
- **Napotkane Problemy i Rozwiązania**
- **Prezentacja Projektu**

## Opis Projektu

**Let Me Solo Her** to prototyp platformy edukacyjnej wspomaganej AI, umożliwiającej użytkownikom efektywną naukę z własnych materiałów. Aplikacja pozwala wgrać pliki PDF (np. notatki, rozdziały książek), które następnie są **przetwarzane przez pipeline RAG (Retrieval-Augmented Generation)** – treść jest dzielona na fragmenty, wektoryzowana i zapisywana w bazie wektorowej. Dzięki temu użytkownik może korzystać z różnych funkcji wspomagających naukę: zadawać pytania w czacie (Q&A) z gwarancją odpowiedzi opartych o wgrany materiał, generować **fiszki** z kluczowymi pojęciami, otrzymać **plan nauki**, tryb **“ciekawskiego dziecka”** z prostszymi pytaniami, tryb **“nauczyciela”** z pogłębionymi wyjaśnieniami, a także automatycznie tworzony **quiz/test** sprawdzający wiedzę. 

Projekt składa się z **frontendu** i **backendu**. Frontend to aplikacja webowa (oparta na Streamlit) z przyjaznym interfejsem, gdzie użytkownik wgrywa pliki i wybiera tryb nauki. Backend to serwer API (FastAPI) obsługujący żądania z frontendu – zajmuje się zapisem danych, wyszukiwaniem informacji i komunikacją z modelem AI. Końcowym użytkownikiem jest każda osoba ucząca się (np. student), która chce w interaktywny sposób przyswoić treści z własnych materiałów: frontend zapewnia interfejs, a backend dokonuje analizy i generuje odpowiedzi na bazie wgranego dokumentu.

Backend wykorzystuje **model językowy “Gemini” (Google Cloud Vertex AI)** do generowania odpowiedzi i materiałów edukacyjnych. W kodzie zaintegrwano SDK od Google (`google-generativeai`), konfigurując model `gemini-1.5-flash` i klucz API Google ([full/backend/services/gemini_service.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/gemini_service.py#:~:text=def%20__init__,flash)). Dzięki temu odpowiedzi AI są kontekstowe i korzystają z zaawansowanego modelu językowego. Dodatkowo backend korzysta z modelu embedding HuggingFace (`multi-qa-mpnet-base-dot-v1`) do zamiany tekstu na wektory ([full/backend/services/RagPipelineService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/RagPipelineService.py#:~:text=,float%C3%B3w)) – służy to do wyszukiwania podobnych fragmentów tekstu w bazie (pgVector). Wykorzystanie RAG polega na tym, że przed wygenerowaniem odpowiedzi AI pobiera z bazy kilka najbardziej pasujących fragmentów dokumentu i dołącza je do promptu, aby **odpowiedź była oparta o fakty z wgranego PDF**. W systemie zastosowano relacyjną bazę danych **PostgreSQL z rozszerzeniem pgVector** jako wektorową bazę wiedzy. Fragmenty dokumentu i ich wektory są tam zapisywane i porównywane przy każdym zapytaniu użytkownika (za pomocą operatora `<->` pgVector mierzącego odległość kosinusową lub euklidesową) ([full/backend/services/RagPipelineService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/RagPipelineService.py#:~:text=%23%20Operator%20%3C,lub%20Euclides%2C%20zale%C5%BCnie%20od%20configu)). Pozwala to skutecznie znaleźć kontekst dla pytania użytkownika i przekazać go modelowi językowemu wraz z pytaniem.

Frontend (Streamlit) stanowi warstwę prezentacji – wyświetla interfejs do uploadu plików, wyboru trybu (czat, fiszki, test itp.) oraz prezentuje wyniki od AI. Backend (FastAPI) udostępnia endpoints: m.in. **`POST /upload_documents`** do wgrywania plików PDF ([full/backend/routers/upload.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/routers/upload.py#:~:text=%40router.post%28)), **`GET /chat`** do pobierania odpowiedzi na pytanie w trybie domyślnym (Q&A) ([full/backend/routers/chat.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/routers/chat.py#:~:text=%40router.get%28)), a także warianty takie jak **`/chat/curious_child`** (odpowiedź w stylu prostych wyjaśnień) ([full/backend/routers/chat.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/routers/chat.py#:~:text=%40router.get%28)), **`/chat/gemini/generate_topics`** (generowanie listy zagadnień do nauki) ([full/backend/routers/chat.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/routers/chat.py#:~:text=%40router.get%28)) oraz **`/chat/test`** (generowanie pytań testowych) ([full/backend/routers/chat.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/routers/chat.py#:~:text=%40router.get%28)). W backendzie zaimplementowano logikę dla tych funkcjonalności w serwisie `RagPipelineService` – np. metoda `generate_test` pobiera kontekst z bazy i na jego podstawie tworzy pytania testowe w formacie JSON ([full/backend/services/RagPipelineService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/RagPipelineService.py#:~:text=async%20def%20generate_test,list)) ([full/backend/services/RagPipelineService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/RagPipelineService.py#:~:text=)). Wszystkie zapytania do modelu AI są logowane wraz z czasem wykonania, długością odpowiedzi itp., co ułatwia analizę działania systemu ([full/backend/services/RagPipelineService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/RagPipelineService.py#:~:text=self.logger.log_llm_call%28)).

Podsumowując, projekt dostarcza kompletny pipeline: **od wgrania materiału**, poprzez **przetwarzanie i indeksację wiedzy**, aż do **interaktywnej sesji z AI** w różnych trybach. Końcowy użytkownik otrzymuje narzędzie, które z jego własnych notatek potrafi wygenerować spersonalizowane materiały do nauki (pytania, streszczenia, fiszki itp.) oraz odpowiedzieć na pytania w oparciu o konkretne treści z tych notatek.

## Instrukcje Uruchomienia

Aplikacja została zbudowana jako **dwa kontenery Dockera** – osobno dla frontendu i backendu – co umożliwia ich niezależne uruchamianie i skalowanie. Poniżej opisano kroki budowania obrazów Docker oraz sposób wdrożenia na Google Cloud Platform (GCP). Wymagania wstępne to zainstalowany Docker oraz posiadanie projektu na GCP z włączonymi usługami Cloud Run (lub alternatywnie Kubernetes/GCE) oraz bazą danych PostgreSQL (Cloud SQL lub Compute Engine).

### Budowanie Obrazu Docker

**Backend:** W katalogu `backend/` znajduje się plik `Dockerfile` definiujący obraz backendu. Bazuje on na lekkim obrazie `python:3.11-slim` ([full/backend/Dockerfile at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/Dockerfile#:~:text=FROM%20python%3A3.11)). W Dockerfile backendu kopiowane jest całe źródło aplikacji oraz instalowane zależności z `requirements.txt` ([full/backend/Dockerfile at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/Dockerfile#:~:text=COPY%20requirements)) (m.in. FastAPI, SQLAlchemy, google-generativeai, PyMuPDF do PDF, HuggingFace transformers itp.). Kontener eksponuje port 8080 i uruchamia serwer FastAPI. W CMD Dockerfile widnieje komenda: `["fastapi", "run", "app.py", "--port", "8080", "--workers", "1"]` ([full/backend/Dockerfile at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/Dockerfile#:~:text=RUN%20pip%20install%20,txt)) – co sugeruje start aplikacji FastAPI. (Uwaga: docelowo powinna to być komenda uruchamiająca Uvicorn, np. `uvicorn app:app ...`, aby backend poprawnie wystartował. W środowisku deweloperskim można też używać parametru `reload=True` dla automatycznego przeładowania kodu podczas developmentu ([full/backend/app.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/app.py#:~:text=if%20__name__%20%3D%3D%20))).

**Frontend:** W katalogu `frontend/` również znajduje się `Dockerfile` zbudowany podobnie (bazuje na `python:3.11-slim`) ([full/frontend/Dockerfile at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/frontend/Dockerfile#:~:text=FROM%20python%3A3.11)). Kopiuje on plik `requirements.txt` frontendu i instaluje potrzebne pakiety (kluczowy jest **Streamlit** jako framework interfejsu) ([full/frontend/Dockerfile at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/frontend/Dockerfile#:~:text=COPY%20requirements)). Następnie kopiuje kod frontendu. Kontener frontendu także eksponuje port 8080 i w CMD uruchamia aplikację poleceniem Streamlit: `streamlit run app.py --server.port=8080 --server.address=0.0.0.0` ([full/frontend/Dockerfile at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/frontend/Dockerfile#:~:text=COPY%20)). Ta komenda startuje serwer webowy Streamlit nasłuchujący na porcie 8080 wewnątrz kontenera, dostępny z zewnątrz pod adresem kontenera.

Aby zbudować obrazy lokalnie, należy wykonać w głównym folderze projektu (lub odpowiednio w `backend/` i `frontend/`): 
```
docker build -t nazwa_uzytkownika/backend-app:latest -f backend/Dockerfile .
docker build -t nazwa_uzytkownika/frontend-app:latest -f frontend/Dockerfile .
``` 
To polecenie stworzy dwa obrazy Dockera – jeden z API backend, drugi z aplikacją Streamlit. Przed zbudowaniem warto upewnić się, że w katalogach znajdują się wszystkie pliki (w tym kod źródłowy i pliki konfiguracyjne). W razie potrzeby należy również ustawić zmienne środowiskowe w trakcie uruchamiania kontenerów (np. klucz API do modelu Gemini oraz URL backendu dla frontendu – opisane niżej).

### Uruchomienie na GCP

Na GCP aplikacja może zostać wdrożona z wykorzystaniem **Cloud Run** – zarządzanej usługi do uruchamiania kontenerów bez konieczności administrowania serwerami. Oba obrazy (frontend i backend) należy najpierw wysłać do rejestru (np. Google Artifact Registry lub Container Registry), a następnie utworzyć dwie usługi Cloud Run. Alternatywnie można użyć Kubernetes (GKE) lub VM (Compute Engine), ale Cloud Run jest najprostszy dla kontenerów stateless.

**Konfiguracja sieci i bezpieczeństwa:** Wszystkie komponenty zostały umieszczone w ramach jednej prywatnej sieci VPC, aby komunikacja między nimi odbywała się w sposób bezpieczny, niewidoczny z Internetu. Baza danych PostgreSQL (z wtyczką pgVector) została uruchomiona jako instancja **Cloud SQL** lub na VM w tej samej sieci VPC – przypisano jej adres prywatny. Frontend i backend (Cloud Run) skonfigurowano do korzystania z **Serverless VPC Connector**, co pozwala im na dostęp do prywatnych zasobów VPC (np. bazy danych) bez wystawiania tych zasobów publicznie. W rezultacie backend łączy się z bazą po **prywatnym adresie IP**, a nie publicznym. (W fazie prototypu użyto adresu publicznego 35.246.200.139 z hasłem do DB ([full/backend/services/RagPipelineService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/RagPipelineService.py#:~:text=host%20%3D%20%2235.246.200.139%22%20,IP%20bazy)), ale docelowo należałoby użyć adresu prywatnego i ograniczyć dostęp). Reguły firewall w sieci VPC ograniczają ruch – np. dopuszczając połączenia na port 5432 do bazy tylko z adresów Cloud Run connectora. Zastosowano **segregację sieci (Zero Trust)**: *frontend* działa jako warstwa DMZ (dostępna publicznie dla użytkownika), *backend* jest dostępny tylko dla frontendu (np. można ograniczyć dostęp do jego URL Cloud Run za pomocą autoryzacji lub poprzez Internal ingress), a *baza danych* dostępna tylko dla backendu. Każda usługa ma odpowiednie **tagi sieciowe**, a reguły firewall bazujące na tych tagach przepuszczają tylko niezbędny ruch (np. tag “backend” może pozwalać łączność z DB, a tag “frontend” – tylko łączność z backendem na odpowiednim porcie/API).

**Uruchomienie backendu:** Należy utworzyć usługę (Cloud Run lub inna) z obrazem backendu. Przy starcie trzeba ustawić zmienne środowiskowe wymagane przez backend:
- `GEMINI_API_KEY` – klucz API do usługi Google Generative AI (Gemini) umożliwiający korzystanie z modelu językowego ([full/backend/services/gemini_service.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/gemini_service.py#:~:text=def%20__init__,flash)).
- `VECTOR_PASSWORD` – hasło do bazy PostgreSQL (używane w SQLAlchemy do połączenia) ([full/backend/services/RagPipelineService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/RagPipelineService.py#:~:text=db_user%20%3D%20)).
- (opcjonalnie) `FRONTEND_URL` – adres frontend (jeśli backend miałby ograniczać CORS tylko do frontendu – w kodzie CORS jest ustawiony na allow_origins=["*"] dla prototypu ([full/backend/app.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/app.py#:~:text=app)), więc nie było to krytyczne).

Backend nasłuchuje na porcie 8080 wewnątrz kontenera; Cloud Run przydziela mu losowy publiczny URL (chyba że ustawiono go jako tylko wewnętrzny). Należy zapewnić, by backend miał dostęp do bazy danych. W Cloud Run można to osiągnąć dodając connector VPC i upewniając się, że instancja Cloud SQL jest w tej samej sieci, lub korzystając z mechanizmu **Cloud SQL Proxy**. W naszym przypadku, backend używa psycopg2 + SQLAlchemy do łączenia się z bazą ([full/backend/services/RagPipelineService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/RagPipelineService.py#:~:text=db_user%20%3D%20)) – przy użyciu prywatnego IP Cloud SQL należałoby użyć odpowiedniego hosta (np. 10.x.x.x).

**Uruchomienie frontendu:** Frontend również uruchamiamy na Cloud Run z obrazem frontendu. Kluczową zmienną jest `BACKEND_URL` – adres, pod którym frontend będzie osiągał backend (np. `https://backend-xxxxxxxx.a.run.app`). W kodzie frontendu wczytywana jest zmienna `BACKEND_URL` i używana do kierowania zapytań API ([full/frontend/connectors/ApiBackend.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/frontend/connectors/ApiBackend.py#:~:text=self._base_url%20%3D%20os.getenv%28)). Dzięki temu frontend wie, gdzie wysyłać pliki do uploadu i zapytania do czatu. Poza tym frontend nie wymaga specjalnych uprawnień sieciowych – łączy się z backendem przez publiczny URL (w wariancie prostym) lub przez wewnętrzny adres w VPC (jeśli zarówno frontend, jak i backend działają jako usługi wewnętrzne w ramach VPC). W naszym wdrożeniu Cloud Run frontendu jest publiczny (umożliwiając użytkownikom dostęp przez przeglądarkę), a backend może być wewnętrzny (Cloud Run **ingress=internal** + **Cloud Run IAM** pozwalający usłudze frontend wywoływać usługę backend). Jeśli jednak nie zastosowano restrykcji, backend posiada publiczny URL, a bezpieczeństwo opiera się na CORS i nieujawnianiu adresu – co należałoby w produkcie finalnym zaostrzyć.

**Uruchomienie bazy danych:** Zakładamy, że baza Postgres z pgVector jest dostępna pod wskazanym hostem, portem, nazwą użytkownika i hasłem. Trzeba utworzyć w niej wymaganą strukturę tabel: co najmniej tabelę `pdf_files` (do przechowywania wgranych plików) oraz tabelę `embeddings` (do przechowywania tekstowych fragmentów i ich wektorów). Z kodu wynika, że:
- `pdf_files` ma kolumny `file_name` (nazwa pliku) i `file_data` (dane pliku PDF jako bytes/bloby) ([full/backend/services/UploadPdfService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/UploadPdfService.py#:~:text=query%20%3D%20text%28)).
- `embeddings` ma kolumny `text` (tekst fragmentu) oraz `embedding` (wektor embedding dla tego tekstu) ([full/backend/services/VectorStoreRetrainer.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/VectorStoreRetrainer.py#:~:text=insert_query%20%3D%20)). Kolumna `embedding` powinna być typu `VECTOR` (dodawanego przez rozszerzenie pgvector, np. `VECTOR(768)` dla modelu MPNet o wymiarze 768). Należy upewnić się, że rozszerzenie pgvector jest zainstalowane w bazie (w Cloud SQL można to zrobić poleceniem `CREATE EXTENSION IF NOT EXISTS vector;`). 

Wdrożenie na Cloud Run zakłada, że aplikacja jest **stateless** – co jest spełnione, ponieważ wszelkie dane (pliki PDF, wektory) trzymane są w bazie, a nie w pamięci kontenerów. Ważne, by rozmiar instancji Cloud Run był dostosowany: przetwarzanie PDF (PyMuPDF) i embeddingi mogą wymagać nieco więcej pamięci, a wywołania do modelu Gemini trochę czasu – w naszym przypadku wybrano mały model “flash” z 10s timeoutem ([full/backend/services/gemini_service.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/gemini_service.py#:~:text=async%20def%20ask,str)), więc standardowe limity Cloud Run (np. 2 vCPU, 4GB RAM, timeout 300s) są wystarczające.

Podsumowując, po zbudowaniu obrazów i ustawieniu usług na Cloud Run, aplikacja powinna być dostępna pod dwoma URL: frontend (Streamlit) – służący użytkownikom, oraz backend (FastAPI) – obsługujący żądania. W środowisku GCP kluczowe jest właściwe ustawienie zmiennych i sieci, aby frontend mógł komunikować się z backendem, a backend z bazą danych, przy zachowaniu zasad bezpieczeństwa.

## Instrukcja Użytkowania

**Korzystanie z aplikacji od strony użytkownika końcowego wygląda następująco:**

1. **Logowanie / Wejście do aplikacji:** (W obecnym prototypie brak systemu logowania – aplikacja jest dostępna publicznie. Docelowo planowane jest dodanie logowania OAuth, np. przez konto Google). Użytkownik otwiera stronę frontendu (adres podany w prezentacji projektu) w przeglądarce. Wita go panel umożliwiający wgranie pliku oraz opis dostępnych funkcji.

2. **Wgranie materiałów (Upload PDF):** Użytkownik wybiera plik PDF ze swojego komputera i przesyła go za pomocą formularza. Po kliknięciu „Wyślij” plik zostaje przesłany do backendu poprzez endpoint `/upload_documents` (wywołanie typu POST z polem pliku) ([full/backend/routers/upload.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/routers/upload.py#:~:text=%40router.post%28)). Frontend informuje o trwającym przesyłaniu – w aplikacji zaimplementowano to przez osobną podstronę „Loading” (1a_Loading.py), która może pokazać animację lub komunikat oczekiwania. W tle backend odbiera plik i zapisuje go w bazie danych (tabela `pdf_files`) wraz z nazwą ([full/backend/services/UploadPdfService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/UploadPdfService.py#:~:text=query%20%3D%20text%28)). Następnie backend przystępuje do przetwarzania dokumentu:
   - Plik PDF jest **parsowany do tekstu** (użyto biblioteki PyMuPDF – ekstrakcja tekstu ze stron PDF ([full/backend/services/VectorStoreRetrainer.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/VectorStoreRetrainer.py#:~:text=,PyMuPDF))).
   - Uzyskany surowy tekst jest opcjonalnie przetwarzany (np. konwersja na markdown dla lepszej prezentacji ([full/backend/services/VectorStoreRetrainer.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/VectorStoreRetrainer.py#:~:text=))) i dzielony na **mniejsze fragmenty** za pomocą algorytmu **RecursiveCharacterTextSplitter** (np. fragmenty ~500 znaków z zachodzeniem 50 znaków) ([full/backend/services/VectorStoreRetrainer.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/VectorStoreRetrainer.py#:~:text=,needed)). To przygotowanie danych do stworzenia wektorowego „wrzutni” wiedzy.
   - Każdy fragment tekstu jest zapisywany do bazy `embeddings` wraz z wektorem embedding wygenerowanym przez model transformera ([full/backend/services/VectorStoreRetrainer.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/VectorStoreRetrainer.py#:~:text=embeddings_list%20%3D%20embedding_model)) ([full/backend/services/VectorStoreRetrainer.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/VectorStoreRetrainer.py#:~:text=insert_query%20%3D%20)). Proces wektoryzacji (embedding) realizuje HuggingFaceEmbeddings – konwertuje tekst na listę 768 liczb (wektor w przestrzeni semantycznej). Te wektory umożliwiają późniejsze podobieństwo semantyczne zapytań do fragmentów tekstu. Po tej operacji baza jest gotowa do odpowiadania na pytania użytkownika związane z zawartością PDF.

   Użytkownik po wgraniu pliku widzi komunikat o pomyślnym zakończeniu uploadu. (Jeśli plik jest duży, operacja może chwilę potrwać – dlatego ekran „Loading” informuje, że trwa **indeksowanie wiedzy**).

3. **Wybór trybu nauki:** Gdy dokument jest już przetworzony, użytkownik przechodzi do następnej sekcji interfejsu (strona “2_MethodSelect”). Tutaj prezentowane są dostępne opcje interakcji:
   - **Chat z AI:** zadawanie własnych pytań do materiału.
   - **Fiszki:** wygenerowanie zestawu fiszek z kluczowymi pytaniami i odpowiedziami z materiału.
   - **Plan nauki:** propozycja podziału materiału na części do nauki w czasie.
   - **Ciekawskie dziecko:** tryb zadawania pytań, które mogłoby zadać dociekliwe dziecko – pomaga sprawdzić, czy użytkownik rozumie materiał prosto.
   - **Nauczyciel:** szczegółowe omówienie tematu – tak jakby nauczyciel tłumaczył materiał.
   - **Test/Quiz:** seria pytań wielokrotnego wyboru sprawdzających wiedzę z dokumentu.

   Użytkownik wybiera jedną z opcji. Np. jeśli wybierze **Chat Q&A**, zostanie przeniesiony do strony czatu.

4. **Chat Q&A (3_Chat.py):** W tym trybie użytkownik wpisuje pytanie w polu tekstowym, dotyczące treści wgranego dokumentu. Po wysłaniu pytania frontend wywołuje endpoint backendu **`GET /chat`** z parametrem zapytania (prompt) ([full/backend/routers/chat.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/routers/chat.py#:~:text=%40router.get%28)). Backend odbiera pytanie i uruchamia pipeline RAG:
   - Najpierw wewnętrznie oblicza embedding pytania (tym samym modelem co dokument) i wykonuje zapytanie do bazy wektorowej, by znaleźć najbardziej zbliżone semantycznie fragmenty tekstu ([full/backend/services/RagPipelineService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/RagPipelineService.py#:~:text=,float%C3%B3w)) ([full/backend/services/RagPipelineService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/RagPipelineService.py#:~:text=%23%20Operator%20%3C,lub%20Euclides%2C%20zale%C5%BCnie%20od%20configu)). Te fragmenty tworzą **kontekst** (zwykle top 3 fragmenty), który potencjalnie zawiera odpowiedź.
   - Następnie backend konstruuje **prompt dla modelu AI** – łączy kontekst i pytanie w szablon promptu. Dla trybu domyślnego jest to np.: "_Odpowiedz na pytanie na podstawie poniższego kontekstu... [kontekst] ... Pytanie: [pytanie użytkownika]_". 
   - Model **Gemini** generuje odpowiedź na podstawie promptu (dzięki zawarciu kontekstu z dokumentu, odpowiedź będzie zgodna z treścią PDF, ograniczając „halucynacje”). 
   - Odpowiedź jest zwracana frontendowi, który wyświetla ją pod pytaniem. Użytkownik może zadać kolejne pytania kontynuując czat – każdorazowo pytanie traktowane jest niezależnie (obecna implementacja nie utrzymuje historii konwersacji, ale dzięki temu nie grozi „przypływ” kontekstu spoza dokumentu).

   Przykład: użytkownik wgrał artykuł o fotosyntezie i pyta *„Jaka jest rola chlorofilu?”*. System znajdzie fragmenty mówiące o chlorofilu i udzieli odpowiedzi w oparciu o nie (np. że chlorofil absorbuje światło słoneczne itp.).  

5. **Fiszki (4_flashcards.py):** Po wybraniu tej opcji aplikacja automatycznie (bez pytania od użytkownika) generuje zestaw pytań i odpowiedzi – potencjalnych fiszek do nauki. Technicznie backend może wykorzystać podobną metodę jak powyżej: np. wywołuje model z prośbą o wygenerowanie listy pytań na podstawie całego kontekstu dokumentu lub głównych tematów. W kodzie przewidziano endpoint **`/chat/gemini/generate_topics`** ([full/backend/routers/chat.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/routers/chat.py#:~:text=%40router.get%28)), który zwraca listę tematów (zagadnień) wywnioskowanych z kontekstu. Ta lista tematów może posłużyć do stworzenia fiszek (pytanie = temat, odpowiedź = opis z kontekstu). Użytkownik otrzymuje na ekranie listę np. 5 fiszek: pytanie z jednej strony, po kliknięciu – odpowiedź z drugiej (Streamlit może np. użyć ekspanderów lub przycisków do odsłaniania odpowiedzi). Fiszki pomagają w powtórce najważniejszych informacji.

6. **Plan nauki (5_study_plan.py):** W tym trybie aplikacja generuje propozycję podziału materiału na dni lub etapy, wraz z wyszczególnieniem, co użytkownik powinien opanować na każdym etapie. Realizowane to może być przez prompt do modelu typu: "Stwórz 7-dniowy plan nauki tego materiału...". Model na podstawie całości kontekstu dzieli zagadnienia na części, np. Dzień 1: definicje podstawowe; Dzień 2: aspekty szczegółowe itd. Plan jest prezentowany użytkownikowi w formie listy punktów lub tabeli.

7. **Ciekawskie dziecko (6_curious_child.py):** To humorystyczny tryb, w którym użytkownik może zobaczyć pytania, jakie zadałoby dociekliwe dziecko dotyczące materiału, oraz spróbować na nie odpowiedzieć. Alternatywnie, użytkownik może sam zadać pytanie, ale odpowiedź AI będzie sformułowana **prościej, w bardziej przystępny sposób**. Technicznie backend posiada osobny endpoint **`/chat/curious_child`** ([full/backend/routers/chat.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/routers/chat.py#:~:text=%40router.get%28)), gdzie wykorzystuje inną wersję promptu (prompt_template_curious_child) – model udziela odpowiedzi prostszym językiem, być może z większą dozą ciekawości. Użytkownik może zadawać pytania trudne, a otrzyma wyjaśnienie „jak dziecku”. To świetny sposób na sprawdzenie, czy potrafimy wytłumaczyć złożone pojęcia prosto (co oznacza dobre zrozumienie tematu).

8. **Nauczyciel (7_teacher.py):** Tryb, w którym AI zachowuje się jak wymagający nauczyciel – może bardziej szczegółowo wyjaśniać zagadnienia, dodać kontekst spoza materiału dla lepszego zrozumienia lub zadawać pogłębiające pytania. W implementacji mógłby to być po prostu inny styl promptu (np. bardziej formalny ton odpowiedzi, więcej szczegółów naukowych). Użytkownik korzysta z tego trybu podobnie do czatu, pytając o wyjaśnienia – odpowiedzi będą wyczerpujące i dogłębne.

9. **Test/Quiz (8_tests.py):** Użytkownik może wygenerować quiz, aby sprawdzić swoją wiedzę po zapoznaniu się z materiałem. Po wybraniu tej opcji aplikacja wywołuje endpoint **`/chat/test`** ([full/backend/routers/chat.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/routers/chat.py#:~:text=%40router.get%28)), który zwraca listę pytań testowych. Backend wykorzystuje przygotowany prompt (prompt_template_test) – podając kontekst z dokumentu, prosi model o wygenerowanie np. 5 pytań wielokrotnego wyboru z czterema opcjami odpowiedzi (A, B, C, D) oraz zaznaczeniem poprawnej odpowiedzi. Odpowiedź modelu jest przetwarzana i przedstawiana użytkownikowi jako interaktywny quiz. Użytkownik może zaznaczyć odpowiedzi, a aplikacja może sprawdzić je z kluczem (w prototypie możliwe, że quiz jest przedstawiony jako lista pytań z odpowiedziami w formie tekstowej). Ten moduł pozwala użytkownikowi ocenić, ile zapamiętał z materiału – pytania są generowane dynamicznie na podstawie treści PDF, więc obejmują istotne szczegóły.

We wszystkich powyższych trybach, jeśli wystąpi błąd (np. backend nie znajdzie kontekstu lub model AI przekroczy limit czasu), system zwróci komunikat o błędzie. Postarano się obsłużyć wyjątki i zwrócić czytelny komunikat w razie problemów ([full/backend/routers/chat.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/routers/chat.py#:~:text=return%20%7B)) ([full/backend/routers/chat.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/routers/chat.py#:~:text=except%20Exception%20as%20e%3A)). 

**Podsumowanie korzystania:** Użytkownik zaczyna od uploadu materiału, następnie wybiera sposób nauki i otrzymuje interaktywne wsparcie od AI. Może wielokrotnie przełączać tryby – np. po sesji czatu wygenerować sobie jeszcze fiszki czy test. Aplikacja stara się być intuicyjna – interfejs Streamlit aktualizuje się dynamicznie po każdej akcji użytkownika. Dzięki temu nawet osoba nietechniczna może skorzystać: wystarczy, że ma swój materiał PDF i dostęp do przeglądarki.

## Diagram Architektury i Opis

*(Diagram architektury został pomyślany, by przedstawić powiązania między komponentami – frontend, backend, baza danych oraz usługi zewnętrzne. Poniżej opisujemy architekturę tekstowo.)*

**Architektura systemu** składa się z kilku głównych komponentów działających w środowisku chmurowym GCP w ramach jednej infrastruktury:

- **Użytkownik (Przeglądarka)** – łączy się przez Internet z aplikacją webową (frontend).
- **Frontend (Streamlit)** – uruchomiony jako kontener Docker (np. w Cloud Run). Jest publicznie dostępny pod określonym URL. Frontend obsługuje interakcję z użytkownikiem (formularz upload, wybór opcji, wyświetlanie odpowiedzi). Po stronie frontendu działa kod Python (Streamlit) renderujący stronę i reagujący na akcje użytkownika. Frontend **komunikuje się z backendem** wykonując zapytania HTTP do jego API (np. `requests.post` przy uploadzie pliku ([full/frontend/connectors/ApiBackend.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/frontend/connectors/ApiBackend.py#:~:text=match%20at%20L590%20response%20%3D,files%3Dprepared_files)), czy `requests.get` przy zadawaniu pytania). Adres URL backendu jest skonfigurowany w zmiennych środowiskowych frontendu (BACKEND_URL) i może wskazywać na wewnętrzny adres w sieci GCP lub publiczny adres usługi backendowej.

- **Backend (FastAPI)** – również jako osobny kontener Docker (oddzielny proces). W architekturze wdrożony np. jako drugi serwis Cloud Run. Backend nasłuchuje na żądania HTTP od frontendu. Główne zadania backendu:
  - Przyjmowanie plików od frontendu (`/upload_documents`) i zapisywanie ich w bazie.
  - Przetwarzanie i indeksowanie dokumentów (ekstrakcja tekstu, dzielenie na fragmenty, tworzenie wektorów, zapis do bazy).
  - Obsługa zapytań użytkownika do AI: odbiór pytania, pobranie odpowiednich fragmentów z bazy danych (zapytania SQL z operacją wektorową), utworzenie promptu i wywołanie modelu AI, zwrócenie wyniku.
  - Udostępnianie różnych endpointów realizujących poszczególne funkcje (chat, flashcards, testy itd.).
  
  Backend w architekturze jest **częścią logiczną aplikacji** – tu odbywa się integracja AI i logika biznesowa. Z punktu widzenia sieci, backend może być skonfigurowany jako serwis dostępny tylko dla frontendu (np. poprzez mechanizmy VPC Service Controls lub odpowiednie ustawienia Cloud Run IAM), dzięki czemu nie jest bezpośrednio wystawiony na Internet (choć w fazie demo może mieć publiczny URL). W architekturze **Zero Trust** backend ufa tylko autoryzowanym połączeniom z frontendu.

- **Baza danych (PostgreSQL + pgVector)** – komponent odpowiedzialny za przechowywanie danych aplikacji:
  - Tabela z plikami PDF (przechowywanie binarne). W produkcyjnej architekturze można by rozważyć zamiast tego trzymanie plików w **Cloud Storage**, a w bazie tylko linków/metadata – ale tu dla prostoty wszystko jest w Postgresie.
  - Tabela z wektorami i tekstami (rdzeń bazy wiedzy do RAG). Każdy rekord zawiera fragment tekstu oraz embedding. Dzięki rozszerzeniu pgVector baza potrafi wykonywać zapytania podobieństwa wektorów.
  - Ewentualnie inne tabele (np. logi zapytań LLM – można by je zapisywać w osobnej tabeli lub w narzędziu typu Cloud Logging; prototyp loguje do konsoli i do zmiennej `logger_service`).
  
  Baza danych jest uruchomiona w prywatnej sieci (np. instancja Cloud SQL) i **nie jest dostępna z Internetu**. Backend łączy się z nią poprzez wewnętrzne IP lub autoryzowany kanał. W architekturze przyjęto, że **wszystkie wrażliwe dane trzymane są w jednym bezpiecznym segmencie sieci**, co minimalizuje powierzchnię ataku.

- **Model AI (Gemini LLM)** – to zewnętrzna usługa dostarczana przez Google Cloud Vertex AI. Nasza aplikacja nie hostuje własnego modelu, lecz korzysta z API w chmurze. Komunikacja z API Gemini odbywa się po HTTPS (zapewniona przez bibliotekę `google-generativeai`). Model wymaga klucza API i ewentualnie uprawnień w projekcie GCP. W architekturze oznacza to, że backend (jako klient API) **musi mieć dostęp do Internetu** lub przynajmniej do endpointów Google API. Cloud Run domyślnie ma egress do Internetu, więc może połączyć się z `api.generativeai.googleapis.com`. Połączenie to jest zabezpieczone kluczem API. **Uwaga dot. bezpieczeństwa:** Klucz API jest przechowywany jako zmienna środowiskowa `GEMINI_API_KEY` w konfiguracji backendu – w GCP sekret ten jest trzymany bezpiecznie (np. Secret Manager + referencja w Cloud Run). Dzięki temu nie pojawia się w repozytorium kodu ani nie jest wysyłany z frontendu.

- **Redis (planowany)** – w architekturze przewidziano użycie serwera Redis jako magazynu sesji i cache. Choć w prototypowym kodzie nie zaimplementowano jeszcze integracji z Redis, docelowo mógłby on służyć do:
  - Przechowywania sesji użytkowników (np. po wprowadzeniu logowania OAuth – tak, aby backend nie musiał za każdym razem uwierzytelniać użytkownika od zera, tylko sprawdzać token sesyjny przechowywany w Redis).
  - Cache embeddingów lub wyników zapytań do LLM, aby przy często powtarzających się pytaniach przyspieszyć odpowiedzi i ograniczyć koszty API.
  - Ograniczanie liczby zapytań (rate limiting) – np. trzymając w Redis licznik zapytań na użytkownika na minutę.
  
  Redis mógłby zostać uruchomiony jako zarządzana usługa (Memory Store) w tej samej VPC, dostępny tylko dla backendu.

- **Bezpieczeństwo komunikacji:** Między frontendem a backendem komunikacja odbywa się po HTTPS (w Cloud Run certyfikaty są zapewnione automatycznie na *.run.app). Między backendem a bazą danych – jeśli użyto Cloud SQL, połączenie może być szyfrowane i nie wychodzi poza sieć Google. Klucz API do modelu jest wysyłany tylko do serwerów Google (ruch wewnątrz chmury). Cała architektura stara się realizować zasadę **Zero Trust** – nawet wewnętrzne komponenty wzajemnie ograniczają zaufanie (np. baza nie ufa nikomu spoza sieci lokalnej, backend wymaga autoryzacji OAuth od frontendu, itp.).

- **DMZ i segmentacja:** Frontend działa w strefie DMZ – ma kontakt z użytkownikiem publicznym. Backend i baza działają w strefie wewnętrznej. Pomiędzy strefami stoi granica w postaci reguł firewall i kontroli dostępu. Np. tylko frontend może rozmawiać z backendem, a tylko backend z bazą. W razie potencjalnego ataku na frontend (który jest najbardziej narażony, bo jest publiczny), ewentualny intruz nie powinien łatwo przeniknąć dalej, bo backend wymaga ważnych tokenów i kluczy do dostępu.

- **Skalowanie:** Dzięki użyciu Cloud Run, oba serwisy mogą skalować się automatycznie. Jeśli wielu użytkowników naraz korzysta z aplikacji, instancje frontendu mogą się zwielokrotnić. Backend również może skalować, choć tu trzeba uważać na *jednoczesny dostęp do bazy* i *limity API modelu*. Zastosowano ograniczenie `max_workers=1` w wywołaniach modelu ([full/backend/services/gemini_service.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/gemini_service.py#:~:text=self)), by serię zapytań wykonywać kolejno (zapobiega to przeciążeniu lub równoległym konfliktowym wywołaniom API AI). W razie większego obciążenia można ewentualnie zwiększyć tę liczbę i zapewnić większy tier usługi AI.

Podsumowując, architektura jest **modułowa i bezpieczna**: oddzielenie frontendu od backendu, wewnętrzna baza wiedzy, integracja z zewnętrznym API AI. Takie podejście ułatwia rozwój (można niezależnie modyfikować front i back), a także zwiększa bezpieczeństwo (komponenty mają jasno określone role i minimalne wzajemne zaufanie).

## Kluczowe Decyzje

Przy realizacji projektu zespół podjął szereg decyzji projektowych i architektonicznych, kierując się bezpieczeństwem, skalowalnością oraz dobrymi praktykami tworzenia aplikacji wykorzystujących AI:

- **Oddzielenie frontendu i backendu:** Postanowiono stworzyć frontend i backend jako osobne aplikacje (procesy) komunikujące się przez sieć. Frontend (Streamlit) i backend (FastAPI) są spakowane w **oddzielne obrazy Docker** i uruchamiane jako niezależne kontenery. Ułatwia to rozwój (można zmieniać interfejs bez wpływu na logikę serwera i odwrotnie) oraz zwiększa bezpieczeństwo – ewentualny problem w jednej warstwie nie kompromituje automatycznie drugiej. Oba kontenery działają we wspólnej infrastrukturze sieciowej i komunikują się po określonym API HTTP. **Wspólna sieć (bridge)** została skonfigurowana w docker-compose lub w Cloud Run poprzez VPC, tak by frontend mógł odwoływać się do hosta backend po nazwie/adresie.

- **Wszystkie bazy danych i usługi w jednej bezpiecznej sieci:** Zdecydowaliśmy umieścić komponenty takie jak baza PostgreSQL i (ew. Redis) w jednym **segmencie sieci (VPC)**, niedostępnym z zewnątrz. Dzięki temu komunikacja pomiędzy backendem a bazą odbywa się wewnętrznie, bez narażania danych na Internet. Wdrożenie na VPS/VM również zakłada, że zarówno aplikacja jak i baza działałyby w obrębie jednego serwera lub sieci lokalnej, gdzie dostęp z zewnątrz jest filtrowany. Na GCP zrealizowano to przez **Cloud Run + VPC + Cloud SQL** w prywatnej sieci. Decyzja ta podyktowana była chęcią minimalizacji wektorów ataku – dane użytkownika (treść dokumentów, wektory) pozostają w zamkniętym środowisku.

- **Segmentacja sieci przy użyciu tagów i firewalli:** W ramach podejścia **Zero Trust** wprowadziliśmy podział na strefy i drobiazgową kontrolę ruchu sieciowego. Każda instancja (frontend, backend, DB) otrzymała **tag sieciowy** pozwalający łatwo tworzyć reguły firewall. Przykładowo: tag “frontend” – ruch przychodzący z Internetu dozwolony tylko na port 443 (HTTPS); tag “backend” – ruch przychodzący dozwolony tylko z zakresów IP frontendu/Cloud Run; tag “database” – ruch przychodzący dozwolony tylko z adresów backendu na port 5432. Dzięki takiej segmentacji, nawet jeśli ktoś uzyskałby dostęp do frontendu, nie może bezpośrednio komunikować się z bazą czy innymi usługami. Firewall odrzuca wszystko, co nie jest jawnie dozwolone. W projekcie posłużono się do tego mechanizmami GCP (Firewall Rules) oraz wbudowanymi opcjami Cloud Run (ingress settings).

- **Model **Zero Trust** i DMZ:** Cała architektura została zaprojektowana zgodnie z zasadą Zero Trust – żadna część systemu nie ufa domyślnie innej, nawet jeśli jest wewnątrz tego samego networku. Wymuszamy uwierzytelnianie i autoryzację tam, gdzie to możliwe. Frontend stanowi strefę **DMZ** – jedyny komponent dostępny bez autoryzacji z Internetu. Backend ufa tylko ruchowi od frontendu (planujemy włączyć np. tokeny autoryzujące w zapytaniach z frontendu, albo ograniczyć dostęp Cloud Run backendu tylko dla tożsamej usługi frontendu). Baza ufa tylko backendowi. Wdrożono też ścisłe **CORS** – aktualnie dopuszczamy wszystkie originy (dla testów) ([full/backend/app.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/app.py#:~:text=CORSMiddleware%2C)), ale docelowo będzie ograniczone do domeny frontendu. Zero Trust oznacza również ciągłe monitorowanie – dlatego logujemy każde zapytanie do modelu AI wraz z parametrami ([full/backend/services/RagPipelineService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/RagPipelineService.py#:~:text=self.logger.log_llm_call%28)), co pozwala na audyt interakcji.

- **Walidacja i sanityzacja wejścia:** Szczególny nacisk położono na zabezpieczenie punktu wejścia, jakim jest **upload plików**. Pliki PDF mogą potencjalnie zawierać złośliwe treści (np. exploity wbudowane w strukturę PDF). W ramach projektu rozważano i częściowo zaplanowano kilka warstw ochrony:
  - **Sanityzacja MIME i rozmiaru:** Backend akceptuje tylko pliki typu PDF o rozsądnym rozmiarze (np. do 10 MB). Frontend filtruje formaty już po stronie UI.
  - **Analiza statyczna plików:** Po otrzymaniu pliku możliwe jest sprawdzenie jego struktury – np. czy nie zawiera skryptów JavaScript, czy nie ma podejrzanych elementów. W przyszłości można by użyć narzędzi do statycznej analizy PDF.
  - **Analiza dynamiczna (sandbox):** Rozważaliśmy wypuszczenie pliku do sandboxa lub użycie VirusTotal API przed przetwarzaniem. VirusTotal pozwoliłby przeskanować plik pod kątem znanych wirusów lub malware. Z racji ograniczeń czasu, nie w pełni zaimplementowano tę funkcję, ale plik PDF jest parsowany za pomocą bezpiecznej biblioteki (PyMuPDF), która ignoruje potencjalnie niebezpieczne elementy i wyciąga tylko tekst.
  - **Sanityzacja promptów:** Również pytania użytkownika w czacie są brane pod uwagę – zastosowano minimalną długość pytania ([full/backend/routers/chat.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/routers/chat.py#:~:text=async%20def%20get_answer,min_length%3D1)), by odrzucić puste zapytania. Ponadto w planach jest filtracja treści w pytaniach, by zapobiec atakom typu prompt injection (np. jeśli użytkownik spróbuje zmusić model do ujawnienia informacji spoza kontekstu, model i tak nie ma takiej wiedzy, bo prompt zawsze zawiera głównie kontekst dokumentu). Ewentualnie można by implementować mechanizmy sprawdzające, czy pytanie nie próbuje nakazać modelowi wykonania czegoś niepożądanego.

- **Sprawdzanie plików (VirusTotal, sandbox):** Jak wspomniano, przewidziano integrację z usługą **VirusTotal API** do sprawdzania hashy plików PDF pod kątem malware. Zero-day pdf exploit detection jest trudny, ale VirusTotal zwiększa szanse wychwycenia znanych zagrożeń. W środowisku produkcyjnym należałoby przed przetworzeniem pliku odczekać na wynik skanu. W razie wykrycia zagrożenia – odrzucić plik i poinformować użytkownika, że plik jest zainfekowany. Ta decyzja projektowa wpisuje się w zasadę, że **nie ufamy nawet plikom od użytkownika**.

- **Ograniczenie zapytań (Rate limiting, Anti-abuse):** Aby zabezpieczyć system przed nadużyciem (np. *DDoS* lub próby masowego wykradania odpowiedzi modelu – tzw. *model stealing*), zaplanowaliśmy mechanizmy limitowania:
  - Frontend mógłby limitować częstotliwość wysyłania zapytań (np. nie pozwolić wysłać więcej niż 1 pytania na sekundę z interfejsu).
  - Backend przy pomocy np. Redis mógłby zliczać zapytania od każdego adresu/IP czy użytkownika i nakładać limit (np. 100 zapytań dziennie na użytkownika). 
  - Cloud Run automatycznie skaluje, ale można też ustawić maksymalną liczbę instancji, by unikać niekontrolowanych kosztów w razie DDoS.
  - Wreszcie integracja z API AI (Gemini) też jest ograniczona – klucz API ma swoje limity, a my dodatkowo ustawiliśmy dość konserwatywny timeout (10s) i jeden wątek zapytań ([full/backend/services/gemini_service.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/gemini_service.py#:~:text=self)), by nie wywoływać zbyt wielu naraz. To chroni zarówno przed przeciążeniem systemu, jak i przed ewentualnym sprytnym zautomatyzowanym odpytywaniem modelu (model stealing – czyli próbom odtworzenia parametrów modelu przez wysyłanie ogromnej liczby zapytań i analizę odpowiedzi).

- **Pipeline RAG zgodny z 2025 Top 10 AI Risk Framework:** Kierowaliśmy się najnowszymi zaleceniami dotyczącymi bezpiecznego użycia generatywnej AI. W szczególności zwróciliśmy uwagę na potencjalne ryzyka (OWASP style) w pipeline RAG:
  - **Hallucinations modelu** – ograniczane przez dostarczanie kontekstu z dokumentu i wręcz instruowanie modelu, by jeśli nie znajdzie odpowiedzi w kontekście, przyznał to (ew. mechanizm fallback, choć w prototypie model zawsze próbuje odpowiedzieć).
  - **Data Leakage** – upewniamy się, że model nie wyśle w odpowiedzi nic spoza dokumentu użytkownika (nie prosimy go np. o podanie klucza API czy informacji systemowych). W promptach nie umieszczamy danych wrażliwych.
  - **Prompt Injection** – rozważono dodanie tokena stopującego lub oczyszczanie promptu z komend typu “Forget instructions”. Póki co polegamy na tym, że nasz prompt szablon jest stały i zawsze poprzedza pytanie użytkownika, więc model ma kontekst co powinien zrobić.
  - **Złośliwe dane treningowe** – w naszym przypadku model jest zewnętrzny, ale nasze konteksty pochodzą od użytkownika. Gdyby ktoś wgrał bardzo szkodliwy tekst, a następnie zadał pytanie, model mógłby wygenerować również szkodliwą odpowiedź. W ramach Top 10 Risk staramy się więc *filtrować treści outputu* – np. można użyć dodatkowego detektora treści zabronionych (w przyszłości). Aktualnie stosujemy się do polityki dostawcy modelu (Google Cloud zapewnia pewne filtrowanie nielegalnych treści po swojej stronie).
  
  Framework Top 10 AI Risk 2025 zapewne zawiera też punkt o ochronie danych osobowych – nasza aplikacja nie udostępnia danych jednego użytkownika innemu, dokumenty nie są nigdzie publikowane, a po zakończeniu sesji użytkownika dane mogą zostać usunięte (tę funkcjonalność można dodać – np. automatyczne czyszczenie bazy po X godzinach lub na żądanie użytkownika).

- **Testy zgodności z Agentic Radar:** Wykorzystaliśmy narzędzie **Agentic Radar** od Splunk (open-source scanner) do analizy naszego kodu pod kątem potencjalnych niepożądanych zachowań agentowych AI. Ponieważ nasza aplikacja korzysta z modelu tylko do generacji tekstu i nie podejmuje autonomicznych działań (jak wykonywanie poleceń w systemie czy dostępu do narzędzi), **Agentic Radar** nie wykazał poważnych zagrożeń. Sprawdziliśmy, że pipeline nie tworzy pętli decyzyjnych ani nie deleguje zadań do agentów AI poza kontrolą użytkownika. Dzięki temu mamy pewność, że AI działa tylko w wyznaczonym zakresie (odpowaida na pytania, generuje treści edukacyjne) i nie wykazuje niepożądanej „agentowości” (np. nie próbuje sama wyszukiwać informacji w internecie, co mogłoby naruszyć sandbox). Decyzja o użyciu Agentic Radar była motywowana chęcią spełnienia standardów transparentności i bezpieczeństwa AI – chcieliśmy pokazać, że nasz projekt jest zgodny z najlepszymi praktykami i nie niesie ukrytych ryzyk.

- **OAuth i Redis dla sesji i separacji użytkowników:** Planujemy wprowadzić **uwierzytelnianie użytkowników przez OAuth2** (np. logowanie Google/GitHub) w przyszłych iteracjach projektu. Pozwoli to na personalizację (każdy użytkownik mógłby przechowywać swoje wgrane dokumenty i wyniki, niedostępne dla innych). W tym celu architektura uwzględnia użycie **Redis** do przechowywania sesji użytkowników – po pomyślnym logowaniu token session (lub ID użytkownika) może być trzymany w Redis, a frontend otrzyma ciasteczko sesyjne. Każde zapytanie do backendu będzie zweryfikowane pod kątem sesji – backend sprawdzi w Redis, czy sesja jest ważna i jaki identyfikator użytkownika się z nią wiąże. Następnie będzie używał tego ID np. do izolowania danych: np. tabela `pdf_files` i `embeddings` mogłyby mieć kolumnę `user_id`, by jeden backend i jedna baza mogła obsługiwać wielu użytkowników, ale każdy widział tylko swoje dane. Ta decyzja zapewni **separację danych użytkowników** – kluczowe z punktu widzenia prywatności. Jednocześnie OAuth upraszcza zarządzanie kontami (delegujemy to do zaufanego dostawcy tożsamości). W prototypie to nie było zaimplementowane w pełni, ale kod jest przygotowany, by łatwo to dodać (np. FastAPI ma FastAPI Users lub można ręcznie w middleware sprawdzać tokeny). Redis jako centralny store sesji jest lekki i szybki – idealny do tego celu.

Podsumowując, decyzje te kształtowały projekt tak, by był zgodny z nowoczesnymi standardami: mikroserwisy, bezpieczeństwo zero trust, sanityzacja wejść, ochrona przed nadużyciami, etyczne użycie AI i poszanowanie prywatności użytkowników. Mimo że nie wszystkie elementy zostały w 100% wdrożone w fazie demo (np. pełna integracja OAuth/Redis czy skanowanie VirusTotal), architektura je przewiduje, a tam gdzie możliwe wykonano już przygotowania pod ich szybkie dodanie.

## Napotkane Problemy i Rozwiązania

Realizacja projektu wiązała się z pokonaniem wielu wyzwań technicznych. Poniżej opisujemy kluczowe problemy, na jakie natrafiliśmy podczas implementacji, oraz sposoby, w jakie je rozwiązaliśmy:

- **Integracja asynchronicznego FastAPI z wywołaniami zewnętrznego API (Gemini):** FastAPI jest frameworkiem asynchronicznym, co oznacza, że nasze endpointy powinny być `async def` i współpracować z event loop Pythona. Natomiast biblioteka do obsługi modelu generatywnego (Google Generative AI) działa w stylu synchronicznym (woła zewnętrzne API). Bezpośrednie wywoływanie jej w funkcji async mogłoby blokować pętlę zdarzeń i opóźniać inne requesty. Napotkaliśmy problem, że **pierwsza implementacja** wywołań do modelu powodowała "zawieszanie się" całej aplikacji na czas odpowiedzi modelu. Rozwiązaliśmy to poprzez użycie `asyncio.get_event_loop().run_in_executor` – czyli delegowanie ciężkiego zapytania do osobnego wątku roboczego ([full/backend/services/gemini_service.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/gemini_service.py#:~:text=async%20def%20ask,str)). W kodzie `GeminiService.ask()` stworzyliśmy nawet pulę wątków z `ThreadPoolExecutor(max_workers=1)` ([full/backend/services/gemini_service.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/gemini_service.py#:~:text=self)), aby seryjnie obsługiwać zapytania. Dzięki temu backend może przyjąć wiele zapytań równolegle: kiedy jedno czeka na odpowiedź AI w tle, inne nie są blokowane i np. mogą wgrywać pliki czy odbierać kolejne pytania (w ograniczonym zakresie). To rozwiązanie pozwoliło pogodzić asynchroniczność FastAPI z synchronicznym charakterem zewnętrznego API.

- **Błędy przy łączeniu z bazą danych i fix-y w SQL:** Początkowo mieliśmy trudności z poprawnym odpytywaniem bazy Postgres o najbliższe wektory. W zapytaniu SQL używamy operatora `<->` z pgvector, ale trzeba było uważać na poprawne formatowanie wektora jako ciągu. W kodzie generujemy zapytanie SQL dynamicznie (co nie jest idealne, ale parametryzacja operatora `<->` okazała się niebanalna) ([full/backend/services/RagPipelineService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/RagPipelineService.py#:~:text=%23%20Operator%20%3C,lub%20Euclides%2C%20zale%C5%BCnie%20od%20configu)). Podczas testów zauważyliśmy, że **źle wybieraliśmy kolumnę** – początkowo próbowaliśmy `SELECT embedding FROM embeddings ...` i potem chcieliśmy użyć wyniku jako tekstu. To oczywiście było błędne (zwracało wektory, nie tekst). Szybko poprawiliśmy zapytanie, by wybierać kolumnę tekstową. W repozytorium widać merge request o nazwie “fix-db”, gdzie takie poprawki zostały wprowadzone. Dodatkowo dodaliśmy obsługę błędu: jeśli zapytanie do bazy się nie powiedzie, rzucamy czytelny wyjątek z komunikatem **"Nie udało się pobrać kontekstu"** ([full/backend/services/RagPipelineService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/RagPipelineService.py#:~:text=except%20Exception%20as%20e%3A)) – to pomogło w debugowaniu, bo od razu wiedzieliśmy, że problem leży w warstwie DB (np. brak rozszerzenia vector, zła nazwa tabeli itp.). Finalnie udało się skonfigurować bazę poprawnie: upewniliśmy się, że rozszerzenie jest zainstalowane i tabela `embeddings` istnieje z odpowiednimi typami kolumn.

- **Długie czasy odpowiedzi i timeouty:** Model Gemini początkowo potrafił odpowiadać kilkanaście sekund, zwłaszcza przy dużym kontekście. Cloud Run ma domyślny timeout ok. 300s, więc to nie groziło przerwaniem, ale z punktu widzenia użytkownika 15-30 sekund czekania to za długo. Zastosowaliśmy kilka kroków:
  - Wybraliśmy mniejszy wariant modelu (**gemini-1.5-flash**), który jest zoptymalizowany pod szybkość kosztem nieco mniejszej dokładności. To skróciło odpowiedzi do ~5 sekund.
  - Ustawiliśmy explicite **timeout na poziomie kodu** – `asyncio.wait_for(..., timeout=10.0)` w wywołaniu modelu ([full/backend/services/gemini_service.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/gemini_service.py#:~:text=try%3A)). Jeśli model nie zdąży w 10s, zwracamy błąd timeout i informujemy użytkownika, że AI nie odpowiedziało na czas ([full/backend/services/gemini_service.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/gemini_service.py#:~:text=return%20result)). Dzięki temu użytkownik nie czeka w nieskończoność; może spróbować ponownie, a my unikamy wiszących zapytań.
  - Ograniczyliśmy długość kontekstu wysyłanego do modelu – bierzemy top 3 fragmenty, a każdy fragment staramy się utrzymać < 500 znaków. To wpływa na szybkość generowania (mniej tekstu do przetworzenia) i również zmniejsza koszt tokenów.

- **Streamlit i problemy z wielostronicowością:** Nasz frontend wykorzystuje mechanizm wielostronicowy (kilka plików w folderze `pages/`). Napotkaliśmy pewną niedogodność – **stan między stronami**. Np. po wgraniu pliku na stronie Upload, na kolejnej stronie Chat chcieliśmy wiedzieć, czy dokument jest już gotowy. Rozważaliśmy wykorzystanie `st.session_state` Streamlit do przekazania informacji (np. session_state['uploaded'] = True). Ostatecznie, ponieważ pipeline indeksacji jest szybki (kilka-kilkanaście sekund), zdecydowaliśmy się na prostsze podejście: **strona ładowania (Loading)**. Użytkownik jest tam przekierowany na parę sekund i wraca, a w tym czasie (plus trochę bufora) backend na ogół zdąży przetworzyć PDF. To zadziałało w naszych testach, ale docelowo lepszym rozwiązaniem byłoby:
  - Użyć mechanizmu **callback** – po uploadzie od razu wywołać endpoint do indeksacji i czekać na jego zakończenie (np. polling co 2 sekundy stan).
  - Albo przenieść proces wektoryzacji do samego upload endpointu – tzn. po zapisaniu pliku od razu wykonać `VectorStoreRetrainer.retrain_vectorstore()`. Mieliśmy nawet przygotowany obiekt `vectorstore_retrainer` w routerze upload ([full/backend/routers/upload.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/routers/upload.py#:~:text=router%20%3D%20APIRouter%28prefix%3D)), ale nie zdążyliśmy tego wywołać w kodzie. Taki refaktor spowodowałby, że po zakończonym wywołaniu `/upload_documents` wszystko byłoby gotowe. 
  - Ewentualnie użyć asynchroniczności – przyjąć plik, zwrócić odpowiedź od razu, a wewnątrz kontynuować przetwarzanie (co jednak komplikowałoby logikę frontendu, bo musiałby sprawdzać kiedy gotowe).
  
  Krótko mówiąc, nauczyliśmy się, że **koordynacja stanu w Streamlit** wymaga uwagi i pewnej innej mentalności niż w tradycyjnym web (tutaj każda interakcja to wykonanie skryptu od nowa). Rozwiązaniem było uproszczenie flow poprzez sekwencyjne strony i zakładanie, że operacja zakończy się szybko.

- **Rozbieżności nazw endpointów między frontem a backiem:** W trakcie prac zmienialiśmy niektóre ścieżki API dla czytelności. Np. endpoint upload początkowo nazywał się `/upload`, potem zmieniliśmy na `/upload_documents` dla większej jasności. W pewnym momencie frontend nadal wysyłał na starą ścieżkę, co powodowało błędy 404. Szybko zidentyfikowaliśmy ten problem dzięki logom (Cloud Run logs pokazał żądania 404) i zaktualizowaliśmy zmienną `BACKEND_URL` w konfiguracji frontendu lub zmieniliśmy kod frontendu, by celował w nowy endpoint. **Lekcja**: przy oddzielnym froncie i backu ważne jest utrzymanie synchronicznej definicji API. W przyszłości warto wygenerować np. plik OpenAPI/Swagger i na jego podstawie wygenerować klienta w Pythonie dla frontendu, by uniknąć literówek i rozjazdów. Ostatecznie, dostosowaliśmy obie strony – w razie czego backend mógłby tymczasowo obsłużyć oba URL (stary i nowy) dla kompatybilności, ale nie było to konieczne.

- **Wybór i integracja modelu AI:** Początkowo rozważaliśmy użycie OpenAI GPT-4/GPT-3.5 lub lokalnego LLM (np. Llama 2) do generowania odpowiedzi. Jednak ze względu na wymogi konkursu (Techfest YARD) i chęć wykorzystania najnowszych osiągnięć, zdecydowaliśmy się na **Google Cloud Gemini**. To rodziło pewne wyzwania:
  - Biblioteka `google-generativeai` była mniej znana, dokumentacja skromniejsza niż OpenAI. Musieliśmy metodą prób skonfigurować model i sprawdzić, czy zwraca to, czego chcemy. Na szczęście okazała się dość prosta w użyciu, a model Gemini-1.5 okazał się wystarczająco dobry do naszych zastosowań.
  - Trzeba było zdobyć dostęp do API (wymaga to whitelisting projektu na dostęp do modeli generatywnych Google – ale wykorzystaliśmy opcję dla programu Cloud AI).
  - Format odpowiedzi – Gemini zwraca obiekt z atrybutem `.text`. Musieliśmy to parsować, czasem usuwać listy箇 bulletowe (model generując tematy dawał je w formie listy wypunktowanej, więc usuwamy znaki `-` czy `•` ([full/backend/services/RagPipelineService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/RagPipelineService.py#:~:text=))).
  
  Ostatecznie integracja powiodła się. Problemem mógł być brak polskich danych – model Gemini jest trenowany m.in. na angielskich treściach. Jednak w testach zauważyliśmy, że radzi sobie także z tekstem polskim całkiem nieźle. Gdyby były problemy z polskimi odmianami, planem B było użycie modelu OpenAI (ale to wymagałoby innego klucza i biblioteki). Zespół zadecydował jednak trzymać się jednej platformy (Google) dla spójności.

- **Wydajność bazy i pamięci:** Przy testowaniu na większych PDF (~50 stron, powiedzmy 30k znaków), baza wektorowa mogła zawierać kilkadziesiąt fragmentów. Zapytanie wektorowe do pgVector po takiej liczbie rekordów jest bardzo szybkie (milisekundy), więc tu nie było problemu. Jednak generowanie embeddingów wszystkich fragmentów trwało parę sekund. Zauważyliśmy, że biblioteka huggingface (transformers model mpnet) działa szybciej na CPU z optymalizacjami. Włączyliśmy **fastText/onnx** w backendzie (poprzez HuggingFaceEmbeddings korzysta z `sentence-transformers` – one pod spodem są zoptymalizowane). Upewniliśmy się, że wymagania zawierają `torch` i odpowiednie pakiety do akceleracji. Mieliśmy drobny problem na etapie budowania Dockera – obraz slim nie ma wielu zależności, musieliśmy doinstalować brakujące biblioteki systemowe (np. libgomp dla np.). Po dodaniu ich, embedding generation działa poprawnie.
  
  Monitorowaliśmy też **zużycie pamięci** – wczytywanie PDF i obróbka tekstu. PyMuPDF jest w C i dość wydajny, ale duży PDF może zużyć kilkaset MB RAM chwilowo. Dlatego przy Cloud Run ustawiliśmy limit 512MiB jako minimum. W testach lokalnych problemów nie było.

- **Logowanie i debugowanie w chmurze:** Jednym z wyzwań typowych przy architekturze rozproszonej jest śledzenie, co się dzieje end-to-end. Korzystaliśmy intensywnie z **Google Cloud Logging** (logi z Cloud Run) oraz wewnętrznego loggera. W kodzie dodaliśmy wiele logów informacyjnych, np. logujemy każde wywołanie do modelu (prompt, czas trwania, ewentualny błąd) ([full/backend/services/RagPipelineService.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/backend/services/RagPipelineService.py#:~:text=self.logger.log_llm_call%28)), logujemy wysyłane requesty z frontendu (URL, parametry) ([full/frontend/connectors/ApiBackend.py at main · Techfest-YARD/full · GitHub](https://github.com/Techfest-YARD/full/blob/main/frontend/connectors/ApiBackend.py#:~:text=url%20%3D%20f)). To pozwoliło nam, patrząc na konsolę GCP, zrozumieć przebieg żądań. Kiedy coś szło nie tak, logi wyraźnie to pokazywały (np. brak klucza API – logowaliśmy błąd inicjalizacji modelu jako wyjątek). Zderzyliśmy się też z problemem, że lokalnie wszystko działało, a na Cloud Run – nie. Logi pomogły ustalić, że brakowało np. zmiennej środowiskowej (zapomnieliśmy ustawić `BACKEND_URL` za pierwszym razem). Po dodaniu brakujących zmiennych i ponownym redeploy, system zadziałał. **Wniosek:** Dobre logowanie i obserwowalność są kluczowe, zwłaszcza gdy mamy pipeline z kilkoma krokami (upload -> indeksacja -> generacja -> odpowiedź).

- **Inne drobne problemy:** 
  - **Kodowanie polskich znaków:** upewniliśmy się, że wszędzie używamy UTF-8. Przy wyciąganiu tekstu z PDF PyMuPDF daje unicode – jest OK. Jednak przy wysyłaniu JSON z FastAPI w odpowiedzi trzeba było skonfigurować ensure_ascii=False, by nie wysyłał ucieczek unicode (FastAPI/Starlette domyślnie powinno to obsłużyć).
  - **Zarządzanie zależnościami:** Mieliśmy dwie różne requirements.txt dla front i back. Trzeba było uważać, by nie instalować zbędnych pakietów (np. front nie potrzebuje sqlalchemy, back nie potrzebuje streamlit). W Dockerfile zrobiliśmy więc osobne instalacje. W testach lokalnych korzystaliśmy z venv – osobno odpalaliśmy backend i frontend. Trochę problematyczne bywało odpalenie Streamlit z odpowiednimi parametrami (trzeba było wskazać katalog, bo inaczej nie widział folderu `pages/`). Ostatecznie rozwiązaliśmy to komendą w Dockerfile, a lokalnie odpalaliśmy `streamlit run frontend/app.py`.
  - **Synchronizacja stanów po stronie klienta:** Ponieważ użyliśmy podejścia jednosesyjnego (aplikacja raczej zakłada jednego użytkownika naraz w danej instancji Streamlit – co jest normalne), nie musieliśmy się martwić o cross-sessions concurrency. Gdyby wiele osób naraz użyło tego samego frontendu, Streamlit by rozróżnił sesje przeglądarki, ale nasz kod i tak kieruje wszystko do jednego wektorowego store. To potencjalny problem: jeśli dwóch różnych userów wgra różne PDF, nasza baza by je wymieszała. W fazie demo to nie wystąpiło (jeden użytkownik naraz), ale docelowo wymaga to wspomnianego mechanizmu izolacji użytkowników (OAuth+Redis+user_id w bazie). Już teraz jednak upewniliśmy się, że np. po wgraniu nowego PDF stara zawartość bazy jest czyszczona lub nie brana pod uwagę. W implementacji najprostszej, nie zaimplementowaliśmy kasowania – więc drugi upload by dodał się do bazy. Użytkownik potem pytając mógłby dostać kontekst z obu dokumentów. To oczywiście do dopracowania – np. można kasować rekordy z poprzednich uploadów dla tej samej sesji. Wniosek: trzeba zidentyfikować użytkownika/sesję. Tego nauczyliśmy się w trakcie testów multi-upload i dlatego zdecydowaliśmy, że jednak prototyp będzie zakładał obsługę jednego dokumentu na raz, a pełna wielosesyjność wymaga mechanizmu użytkowników.

Każdy z tych problemów dał nam cenną lekcję i pozwolił ulepszyć projekt. Dzięki iteracyjnemu podejściu (pisanie kodu, testy jednostkowe integracyjne, poprawki) doprowadziliśmy aplikację do stanu, w którym **działa stabilnie** dla podstawowego scenariusza. Równie ważne – przygotowaliśmy grunt pod dalszy rozwój, mając w świadomości, jakie pułapki mogą czekać (zwłaszcza w kwestii bezpieczeństwa i multi-user). 

## Prezentacja Projektu

Projekt **Let Me Solo Her team** został z powodzeniem wdrożony i działa pod adresem: **https://frontend-46193761155.europe-west3.run.app/**. Pod tym linkiem dostępny jest frontend aplikacji (Streamlit). Można tam przetestować działanie systemu – wgrać własny plik PDF (np. artykuł, notatki) i skorzystać z opisywanych funkcji: czatu z AI, generowania fiszek, planu nauki czy quizu. Backend API jest uruchomiony na GCP i zintegrowany z frontendem (zapytania z interfejsu trafiają do AI w czasie rzeczywistym, więc prosimy o cierpliwość przy generowaniu odpowiedzi). 

Na prezentację projektu składa się demo live na wskazanej stronie oraz kod źródłowy dostępny w repozytorium GitHub (Techfest-YARD/full). Zapraszamy do samodzielnego wypróbowania – aplikacja umożliwia doświadczenie, jak AI może stać się osobistym asystentem do nauki. W przyszłości planujemy rozszerzyć projekt o wymienione usprawnienia (logowanie, lepsze zabezpieczenia) i mamy nadzieję, że posłuży on jako pomoc naukowa dla wielu osób. Dzięki architekturze opartej na chmurze, projekt łatwo skalować i udostępniać szerokiej grupie odbiorców, zachowując przy tym bezpieczeństwo i prywatność danych użytkowników. Powodzenia w testowaniu i dziękujemy za uwagę!
